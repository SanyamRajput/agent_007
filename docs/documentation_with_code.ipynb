{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title",
   "metadata": {},
   "source": [
    "# LangGraph Chatbot - How It Works\n",
    "\n",
    "This notebook explains the working of a conversational AI agent built with LangGraph and Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_explanation",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies\n",
    "\n",
    "We import the necessary components:\n",
    "- **StateGraph**: Core component for building the conversation flow\n",
    "- **Message types**: For structuring conversation data\n",
    "- **ChatOllama**: Interface to the local Ollama model\n",
    "- **TypedDict & Annotations**: For type safety and state management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state_explanation",
   "metadata": {},
   "source": [
    "## 2. Define Agent State\n",
    "\n",
    "The `AgentState` is the data structure that flows through our graph. It contains:\n",
    "- **messages**: A sequence of conversation messages\n",
    "- **Annotated with `add`**: This allows new messages to be appended to existing ones\n",
    "- **Type hints**: Ensures only HumanMessage or AIMessage objects are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agent_state",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[HumanMessage | AIMessage], add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_explanation",
   "metadata": {},
   "source": [
    "## 3. Initialize Language Model and System Prompt\n",
    "\n",
    "- **ChatOllama**: Creates connection to the local Gemma 2B model\n",
    "- **Temperature 0.7**: Controls randomness (0=deterministic, 1=very random)\n",
    "- **SystemMessage**: Sets the AI's personality and behavior guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = ChatOllama(model=\"gemma:2b\", temperature=0.7)\n",
    "\n",
    "sys_prompt = SystemMessage(\n",
    "    content=\"You are a helpful, general-purpose AI assistant. Answer clearly and politely.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "node_explanation",
   "metadata": {},
   "source": [
    "## 4. Define the LLM Node Function\n",
    "\n",
    "This is the core processing function that:\n",
    "1. Takes the current state (conversation history)\n",
    "2. Prepends the system prompt to guide behavior\n",
    "3. Sends all messages to the language model\n",
    "4. Returns the AI's response in the correct state format\n",
    "\n",
    "The function signature matches LangGraph's node requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "llm_node",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_node(state: AgentState) -> AgentState:\n",
    "    messages = [sys_prompt] + list(state[\"messages\"])\n",
    "    response = llm_model.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph_explanation",
   "metadata": {},
   "source": [
    "## 5. Create the Conversation Graph\n",
    "\n",
    "The graph defines the flow of conversation:\n",
    "1. **StateGraph(AgentState)**: Creates a graph that manages our state type\n",
    "2. **add_node**: Registers our LLM function as a processing node\n",
    "3. **set_entry_point**: Defines where conversations start\n",
    "4. **add_edge**: Creates a path from LLM to END (conversation ends after each response)\n",
    "5. **compile()**: Optimizes the graph for execution\n",
    "\n",
    "This creates a simple linear flow: Input → LLM Processing → Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_agent():\n",
    "    graph = StateGraph(AgentState)\n",
    "    graph.add_node(\"llm_model\", llm_node)\n",
    "    graph.set_entry_point(\"llm_model\")\n",
    "    graph.add_edge(\"llm_model\", END)\n",
    "    return graph.compile()\n",
    "\n",
    "bot = create_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interface_explanation",
   "metadata": {},
   "source": [
    "## 6. Interactive Chat Interface\n",
    "\n",
    "This function provides a command-line chat interface:\n",
    "- **Conversation loop**: Continuously accepts user input\n",
    "- **History management**: Maintains context across exchanges\n",
    "- **State updates**: Each message is added to history for context\n",
    "- **Error handling**: Gracefully handles interruptions and errors\n",
    "\n",
    "The bot maintains memory by keeping all previous messages in the history list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run_bot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bot():\n",
    "    print(\"Agent007 Bot - Ready to assist!\")\n",
    "    print(\"Type 'exit' or 'bye' to quit the conversation.\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    history = []\n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nYou: \").strip()\n",
    "            \n",
    "            if user_input.lower() in [\"exit\", \"bye\", \"quit\"]:\n",
    "                print(\"\\n007: Goodbye! Have a great day!\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                print(\"Please enter a message or type 'exit' to quit.\")\n",
    "                continue\n",
    "                \n",
    "            user_msg = HumanMessage(content=user_input)\n",
    "            history.append(user_msg)\n",
    "            \n",
    "            result = bot.invoke({\"messages\": history})\n",
    "            ai_msg = result[\"messages\"][-1]\n",
    "            \n",
    "            print(f\"\\n007: {ai_msg.content}\")\n",
    "            history.append(ai_msg)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n007: Conversation interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Please try again or type 'exit' to quit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single_question_explanation",
   "metadata": {},
   "source": [
    "## 7. Single Question Function\n",
    "\n",
    "This utility function allows asking single questions:\n",
    "- **Optional history**: Can include previous conversation context\n",
    "- **Message wrapping**: Converts string input to proper message format\n",
    "- **Direct response**: Returns just the text content, not the full message object\n",
    "\n",
    "Useful for programmatic interactions or one-off questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ask_bot",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_bot(question: str, conversation_history=None):\n",
    "    if conversation_history is None:\n",
    "        conversation_history = []\n",
    "    \n",
    "    user_msg = HumanMessage(content=question)\n",
    "    messages = conversation_history + [user_msg]\n",
    "    \n",
    "    result = bot.invoke({\"messages\": messages})\n",
    "    return result[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flow_explanation",
   "metadata": {},
   "source": [
    "## 8. How the Data Flows\n",
    "\n",
    "Here's what happens when you send a message:\n",
    "\n",
    "1. **User Input** → Converted to HumanMessage\n",
    "2. **History Update** → Message added to conversation history\n",
    "3. **State Creation** → History wrapped in AgentState format\n",
    "4. **Graph Execution** → bot.invoke() runs the graph\n",
    "5. **LLM Processing** → llm_node() processes the state\n",
    "6. **Model Call** → System prompt + history sent to Gemma\n",
    "7. **Response Generation** → Model generates AIMessage response\n",
    "8. **State Return** → New message returned in state format\n",
    "9. **History Update** → AI response added to conversation history\n",
    "10. **Display** → Response shown to user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo_section",
   "metadata": {},
   "source": [
    "## 9. Demonstration\n",
    "\n",
    "Let's see the bot in action with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_bot(\"Hello! What can you help me with?\")\n",
    "print(f\"Bot: {response}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"To start interactive chat, run: run_bot()\")\n",
    "print(\"To ask single questions, use: ask_bot('your question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture_explanation",
   "metadata": {},
   "source": [
    "## 10. Key Architecture Benefits\n",
    "\n",
    "**LangGraph provides several advantages:**\n",
    "\n",
    "- **State Management**: Automatic handling of conversation context\n",
    "- **Modular Design**: Easy to add new nodes (tools, memory, etc.)\n",
    "- **Type Safety**: Ensures data consistency across the graph\n",
    "- **Scalability**: Can handle complex multi-step reasoning\n",
    "- **Debugging**: Clear visibility into each step of processing\n",
    "\n",
    "**This simple example demonstrates:**\n",
    "- Linear conversation flow\n",
    "- Context preservation across turns\n",
    "- Integration with local LLMs\n",
    "- Clean separation of concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usage_section",
   "metadata": {},
   "source": [
    "## 11. Usage Examples\n",
    "\n",
    "Try these in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_bot()\n",
    "\n",
    "# ask_bot(\"Explain quantum computing in simple terms\")\n",
    "\n",
    "# history = []\n",
    "# response1 = ask_bot(\"My name is Alice\", history)\n",
    "# history.extend([HumanMessage(content=\"My name is Alice\"), AIMessage(content=response1)])\n",
    "# response2 = ask_bot(\"What's my name?\", history)\n",
    "# print(response2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
